{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed7c89-a47e-4417-aab0-b47945f34a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import joblib # Optional, kept for consistency\n",
    "import traceback\n",
    "import warnings\n",
    "import re # For feature filtering if needed\n",
    "import time # Import time for timing\n",
    "import shap # Import SHAP\n",
    "import os # Import os for path manipulation\n",
    "\n",
    "# Use standard tqdm if not in notebook environment\n",
    "try:\n",
    "    from tqdm import tqdm # Use standard tqdm for loops\n",
    "except ImportError:\n",
    "    # Fallback if tqdm is not installed (less pretty output)\n",
    "    def tqdm(iterable, *args, **kwargs):\n",
    "        if hasattr(iterable, '__len__') and len(iterable) > 0:\n",
    "            print(\"Warning: tqdm library not found. Progress bars will not be shown.\")\n",
    "        return iterable\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score\n",
    ")\n",
    "# Import matplotlib for saving SHAP plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress warnings for cleaner output (optional)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================\n",
    "# --- Configuration ---\n",
    "# ==============================================================\n",
    "\n",
    "# !! IMPORTANT: Verify this BASE_PATH is correct !!\n",
    "BASE_PATH = Path(r'C:\\Users\\sdogan') # <--- CHANGE THIS TO YOUR ACTUAL BASE PATH\n",
    "# Or use forward slashes:\n",
    "# BASE_PATH = Path('/path/to/your/base/directory')\n",
    "\n",
    "DATA_DIR = BASE_PATH / 'Reddit_Virality_Data'\n",
    "API_RESULTS_DIR = DATA_DIR / 'API_results'\n",
    "NET_TEMP_PROCESSED_DIR = API_RESULTS_DIR / 'preprocessed_data'\n",
    "EVALUATION_RESULTS_DIR = API_RESULTS_DIR / 'evaluation_results'\n",
    "FEATURE_IMPORTANCE_DIR = EVALUATION_RESULTS_DIR / 'feature_importances'\n",
    "SHAP_RESULTS_DIR = EVALUATION_RESULTS_DIR / 'shap_results'\n",
    "\n",
    "TIME_WINDOWS_TO_PROCESS = [ 30, 60, 120, 180, 240, 300, 360, 420 ]\n",
    "\n",
    "TARGET_COLUMN = 'is_viral'\n",
    "ID_COLUMN = 'id'\n",
    "ORIGINAL_ID_COLUMN = 'original_id'\n",
    "RANDOM_STATE = 42\n",
    "SNAPSHOT_TIME_COLUMN = 'snapshot_time_minutes'\n",
    "\n",
    "INTERMEDIATE_TRAIN_PREFIX = \"train_\"\n",
    "INTERMEDIATE_VALID_PREFIX = \"val_\"\n",
    "INTERMEDIATE_TEST_PREFIX = \"test_\"\n",
    "\n",
    "FINAL_RESULTS_FILENAME = \"RF_LR_baseline_all_windows_RF_ablation_60min.csv\"\n",
    "\n",
    "MODALITY_PREFIX_GROUPS = {\n",
    "    \"contextual\": [\"contextual_\"],\n",
    "    \"temporal\": [\"temporal_\"],\n",
    "    \"visual\": [\"visual_\"],\n",
    "    \"network\": [\"network_\"],\n",
    "    \"textual\": [\"textual_\"],\n",
    "}\n",
    "\n",
    "print(f\"--- Testing RF & LR Baseline (All Windows) & RF Ablation (60min Window) ---\")\n",
    "print(f\"--- Time Windows: {TIME_WINDOWS_TO_PROCESS} ---\")\n",
    "print(f\"--- Using {len(MODALITY_PREFIX_GROUPS)} Modalities for RF Ablation (Custom Temporal Logic) ---\")\n",
    "print(f\"Reading intermediate processed data from: {NET_TEMP_PROCESSED_DIR}\")\n",
    "print(f\"Saving final comparison results to: {EVALUATION_RESULTS_DIR}\")\n",
    "print(f\"Saving baseline feature importance results to: {FEATURE_IMPORTANCE_DIR}\")\n",
    "print(f\"Saving baseline RF SHAP summary results and plots to: {SHAP_RESULTS_DIR}\")\n",
    "\n",
    "EVALUATION_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FEATURE_IMPORTANCE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SHAP_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ==============================================================\n",
    "# --- Helper Function for Saving Importance ---\n",
    "# ==============================================================\n",
    "def save_feature_importance(model, feature_names, model_type, window_minutes, save_dir, scenario=\"baseline\"):\n",
    "    \"\"\"\n",
    "    Calculates, sorts, and saves feature importance/coefficients.\n",
    "    Only saves if scenario is \"baseline\".\n",
    "    \"\"\"\n",
    "    if scenario != \"baseline\":\n",
    "        return\n",
    "\n",
    "    importance_df = None\n",
    "    col_name = 'importance'\n",
    "    try:\n",
    "        if hasattr(model, 'feature_importances_'): # For RF\n",
    "            importances = model.feature_importances_\n",
    "            importance_df = pd.DataFrame({'feature': feature_names, col_name: importances})\n",
    "            importance_df = importance_df.sort_values(by=col_name, ascending=False)\n",
    "        elif hasattr(model, 'coef_'): # For LR\n",
    "            col_name = 'coefficient'\n",
    "            if model.coef_.ndim == 2 and model.coef_.shape[0] == 1:\n",
    "                coefficients = model.coef_[0]\n",
    "                importance_df = pd.DataFrame({'feature': feature_names, col_name: coefficients})\n",
    "                importance_df = importance_df.iloc[importance_df[col_name].abs().argsort()[::-1]]\n",
    "            else:\n",
    "                print(f\"      LR coefficient shape unexpected ({model.coef_.shape}). Skipping importance save.\")\n",
    "                return\n",
    "        else:\n",
    "            print(f\"      Model type {model_type} does not have standard importance/coefficients. Skipping.\")\n",
    "            return\n",
    "\n",
    "        if importance_df is not None:\n",
    "            filename = f\"{model_type}_feature_importance_{window_minutes}min_baseline.csv\"\n",
    "            save_path = save_dir / filename\n",
    "            importance_df.to_csv(save_path, index=False, float_format='%.6f')\n",
    "            # print(f\"      Saved baseline feature importance/coefficients to: {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"      Error saving feature importance for {model_type}, window {window_minutes} min, scenario {scenario}: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# ==============================================================\n",
    "# --- Helper Function for Saving SHAP Summary (RF ONLY) ---\n",
    "# ==============================================================\n",
    "def save_shap_summary(shap_values, feature_names, model_type, window_minutes, save_dir, scenario=\"baseline\"):\n",
    "    \"\"\"\n",
    "    Saves mean absolute SHAP values for RF model in baseline scenario.\n",
    "    \"\"\"\n",
    "    if scenario != \"baseline\" or model_type != 'RF':\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "        if len(mean_abs_shap) != len(feature_names):\n",
    "            print(f\"      Error: Mismatch SHAP values ({len(mean_abs_shap)}) and feature names ({len(feature_names)}). Skipping SHAP save.\")\n",
    "            return\n",
    "\n",
    "        shap_summary_df = pd.DataFrame({'feature': feature_names, 'mean_abs_shap': mean_abs_shap})\n",
    "        shap_summary_df = shap_summary_df.sort_values(by='mean_abs_shap', ascending=False)\n",
    "        \n",
    "        filename = f\"RF_shap_summary_{window_minutes}min_baseline.csv\"\n",
    "        save_path = save_dir / filename\n",
    "        shap_summary_df.to_csv(save_path, index=False, float_format='%.6f')\n",
    "        # print(f\"      Saved baseline RF SHAP summary CSV to: {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"      Error calculating/saving RF SHAP summary for window {window_minutes} min, scenario {scenario}: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# ==============================================================\n",
    "# --- Helper Function for Evaluation ---\n",
    "# ==============================================================\n",
    "def evaluate_model(model, X_eval, y_eval, model_type, window, scenario, eval_set_name):\n",
    "    \"\"\"\n",
    "    Evaluates a trained model.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    # print(f\"        Evaluating {model_type} on {eval_set_name.capitalize()} Set ({scenario}, {window} min)...\")\n",
    "    try:\n",
    "        probs = model.predict_proba(X_eval)[:, 1]\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "\n",
    "        metrics[f'pr_auc_{eval_set_name}'] = average_precision_score(y_eval, probs)\n",
    "        metrics[f'roc_auc_{eval_set_name}'] = roc_auc_score(y_eval, probs)\n",
    "        metrics[f'f1_score_{eval_set_name}'] = f1_score(y_eval, preds, zero_division=0)\n",
    "        metrics[f'precision_{eval_set_name}'] = precision_score(y_eval, preds, zero_division=0)\n",
    "        metrics[f'recall_{eval_set_name}'] = recall_score(y_eval, preds, zero_division=0)\n",
    "        metrics[f'accuracy_{eval_set_name}'] = accuracy_score(y_eval, preds)\n",
    "        # print(f\"          {model_type} {eval_set_name.capitalize()} PR AUC: {metrics[f'pr_auc_{eval_set_name}']:.5f}, ROC AUC: {metrics[f'roc_auc_{eval_set_name}']:.5f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"        Error during {model_type} evaluation on {eval_set_name} for scenario '{scenario}', window {window} min: {e}\")\n",
    "        traceback.print_exc()\n",
    "        for metric_name in ['pr_auc', 'roc_auc', 'f1_score', 'precision', 'recall', 'accuracy']:\n",
    "            metrics[f'{metric_name}_{eval_set_name}'] = np.nan\n",
    "        metrics[f'error_{eval_set_name}'] = str(e)\n",
    "    return metrics\n",
    "\n",
    "# ==============================================================\n",
    "# --- Main Script Logic ---\n",
    "# ==============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    all_results_list = []\n",
    "    script_start_time = time.time()\n",
    "\n",
    "    for window_minutes in tqdm(TIME_WINDOWS_TO_PROCESS, desc=\"Processing Time Windows\", unit=\"window\"):\n",
    "        window_start_time = time.time()\n",
    "        print(f\"\\n===== Processing Time Window: {window_minutes} minutes =====\")\n",
    "\n",
    "        train_filename = f\"{INTERMEDIATE_TRAIN_PREFIX}{window_minutes}min.parquet\"\n",
    "        valid_filename = f\"{INTERMEDIATE_VALID_PREFIX}{window_minutes}min.parquet\"\n",
    "        test_filename = f\"{INTERMEDIATE_TEST_PREFIX}{window_minutes}min.parquet\"\n",
    "        intermediate_train_path = NET_TEMP_PROCESSED_DIR / train_filename\n",
    "        intermediate_valid_path = NET_TEMP_PROCESSED_DIR / valid_filename\n",
    "        intermediate_test_path = NET_TEMP_PROCESSED_DIR / test_filename\n",
    "\n",
    "        df_train, df_valid, df_test = None, None, None\n",
    "        try:\n",
    "            print(f\"  Loading data for window: {window_minutes} min...\")\n",
    "            df_train = pd.read_parquet(intermediate_train_path)\n",
    "            df_valid = pd.read_parquet(intermediate_valid_path)\n",
    "            df_test = pd.read_parquet(intermediate_test_path)\n",
    "            print(f\"    Loaded Train: {df_train.shape}, Valid: {df_valid.shape}, Test: {df_test.shape}\")\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"  WARNING: Data file not found for window {window_minutes} min ({e}). Skipping.\")\n",
    "            error_msg = 'InputFileNotFound'\n",
    "            all_results_list.append({'model_type': 'RandomForestClassifier', 'time_window': window_minutes, 'ablation_scenario': 'baseline', 'error': error_msg})\n",
    "            all_results_list.append({'model_type': 'LogisticRegression', 'time_window': window_minutes, 'ablation_scenario': 'baseline', 'error': error_msg})\n",
    "            if window_minutes == 60:\n",
    "                for modality_name in MODALITY_PREFIX_GROUPS.keys():\n",
    "                    all_results_list.append({'model_type': 'RandomForestClassifier', 'time_window': window_minutes, 'ablation_scenario': f'exclude_{modality_name}', 'error': error_msg})\n",
    "            if 'df_train' in locals(): del df_train; gc.collect()\n",
    "            if 'df_valid' in locals(): del df_valid; gc.collect()\n",
    "            if 'df_test' in locals(): del df_test; gc.collect()\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR loading data for window {window_minutes} min: {e}. Skipping.\")\n",
    "            traceback.print_exc()\n",
    "            error_msg = f'DataLoadError: {e}'\n",
    "            all_results_list.append({'model_type': 'RandomForestClassifier', 'time_window': window_minutes, 'ablation_scenario': 'baseline', 'error': error_msg})\n",
    "            all_results_list.append({'model_type': 'LogisticRegression', 'time_window': window_minutes, 'ablation_scenario': 'baseline', 'error': error_msg})\n",
    "            if window_minutes == 60:\n",
    "                for modality_name in MODALITY_PREFIX_GROUPS.keys():\n",
    "                    all_results_list.append({'model_type': 'RandomForestClassifier', 'time_window': window_minutes, 'ablation_scenario': f'exclude_{modality_name}', 'error': error_msg})\n",
    "            if 'df_train' in locals(): del df_train; gc.collect()\n",
    "            if 'df_valid' in locals(): del df_valid; gc.collect()\n",
    "            if 'df_test' in locals(): del df_test; gc.collect()\n",
    "            continue\n",
    "\n",
    "        X_train, y_train, X_valid, y_valid, X_test, y_test = None, None, None, None, None, None\n",
    "        X_train_scaled, X_valid_scaled, X_test_scaled = None, None, None\n",
    "        all_feature_cols = []\n",
    "        modality_feature_sets = {name: [] for name in MODALITY_PREFIX_GROUPS.keys()}\n",
    "        scaler = None\n",
    "\n",
    "        try:\n",
    "            print(\"  Preparing features (X), target (y), and modality groups...\")\n",
    "            original_id_col_to_use = None\n",
    "            if ORIGINAL_ID_COLUMN in df_train.columns: original_id_col_to_use = ORIGINAL_ID_COLUMN\n",
    "            elif ID_COLUMN in df_train.columns: original_id_col_to_use = ID_COLUMN\n",
    "            \n",
    "            if 'media_type' in df_train.columns and original_id_col_to_use:\n",
    "                audio_extensions = ['.m4a', '.mp3', '.wav', '.aac', '.ogg', '.flac']\n",
    "                for df_split_name, df_split in [(\"Train\", df_train), (\"Valid\", df_valid), (\"Test\", df_test)]:\n",
    "                    if 'media_type' in df_split.columns and original_id_col_to_use in df_split.columns:\n",
    "                        df_split['media_type'] = df_split['media_type'].fillna('Unknown').astype(str)\n",
    "                        df_split[original_id_col_to_use] = df_split[original_id_col_to_use].fillna('').astype(str)\n",
    "                        unknown_media_mask = df_split['media_type'].str.lower().isin(['unknown', ''])\n",
    "                        is_audio_mask = df_split[original_id_col_to_use].str.lower().apply(lambda x: any(x.endswith(ext) for ext in audio_extensions))\n",
    "                        update_mask = unknown_media_mask & is_audio_mask\n",
    "                        if update_mask.sum() > 0:\n",
    "                            df_split.loc[update_mask, 'media_type'] = 'audio'\n",
    "                            # print(f\"      Updated {update_mask.sum()} 'Unknown' media types to 'audio' in {df_split_name} based on extension.\")\n",
    "\n",
    "            exclude_cols = [TARGET_COLUMN, ID_COLUMN, SNAPSHOT_TIME_COLUMN, ORIGINAL_ID_COLUMN, 'media_type'] # Add media_type if it was used for inference\n",
    "            initial_feature_cols = [col for col in df_train.columns if col not in exclude_cols and col not in MODALITY_PREFIX_GROUPS.keys()] # Avoid picking up modality group names if they are columns\n",
    "            \n",
    "            common_features = set(initial_feature_cols).intersection(df_valid.columns).intersection(df_test.columns)\n",
    "            all_feature_cols = [col for col in initial_feature_cols if col in common_features]\n",
    "\n",
    "            if not all(TARGET_COLUMN in df.columns for df in [df_train, df_valid, df_test]):\n",
    "                raise ValueError(f\"Target column '{TARGET_COLUMN}' not found.\")\n",
    "\n",
    "            X_train = df_train[all_feature_cols].fillna(0)\n",
    "            y_train = df_train[TARGET_COLUMN]\n",
    "            X_valid = df_valid[all_feature_cols].fillna(0)\n",
    "            y_valid = df_valid[TARGET_COLUMN]\n",
    "            X_test = df_test[all_feature_cols].fillna(0)\n",
    "            y_test = df_test[TARGET_COLUMN]\n",
    "            print(f\"    Using {len(all_feature_cols)} common features.\")\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_valid_scaled = scaler.transform(X_valid)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            temporal_suffix = f\"_0to{window_minutes}min\"\n",
    "            assigned_features = set()\n",
    "            temporal_features = {col for col in all_feature_cols if col.startswith('temporal_') or col.endswith(temporal_suffix)}\n",
    "            modality_feature_sets['temporal'] = list(temporal_features)\n",
    "            assigned_features.update(temporal_features)\n",
    "\n",
    "            for name, prefixes in MODALITY_PREFIX_GROUPS.items():\n",
    "                if name == 'temporal': continue\n",
    "                features_in_group = set()\n",
    "                for prefix in prefixes:\n",
    "                    matched_cols = {col for col in all_feature_cols if col.startswith(prefix) and col not in assigned_features}\n",
    "                    features_in_group.update(matched_cols)\n",
    "                modality_feature_sets[name] = list(features_in_group)\n",
    "                assigned_features.update(features_in_group)\n",
    "            \n",
    "            unassigned = [col for col in all_feature_cols if col not in assigned_features]\n",
    "            if unassigned: print(f\"    WARNING: {len(unassigned)} features unassigned to modalities: {unassigned[:5]}...\")\n",
    "            \n",
    "            del df_train, df_valid, df_test; gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR preparing data/modalities for window {window_minutes} min: {e}. Skipping.\")\n",
    "            traceback.print_exc()\n",
    "            error_msg = f'PrepareDataError: {e}'\n",
    "            all_results_list.append({'model_type': 'RandomForestClassifier', 'time_window': window_minutes, 'ablation_scenario': 'baseline', 'error': error_msg})\n",
    "            all_results_list.append({'model_type': 'LogisticRegression', 'time_window': window_minutes, 'ablation_scenario': 'baseline', 'error': error_msg})\n",
    "            if window_minutes == 60:\n",
    "                for modality_name in MODALITY_PREFIX_GROUPS.keys():\n",
    "                    all_results_list.append({'model_type': 'RandomForestClassifier', 'time_window': window_minutes, 'ablation_scenario': f'exclude_{modality_name}', 'error': error_msg})\n",
    "            if 'df_train' in locals(): del df_train; gc.collect()\n",
    "            if 'df_valid' in locals(): del df_valid; gc.collect()\n",
    "            if 'df_test' in locals(): del df_test; gc.collect()\n",
    "            if 'X_train' in locals(): del X_train, y_train, X_valid, y_valid, X_test, y_test, X_train_scaled, X_valid_scaled, X_test_scaled, scaler, all_feature_cols, modality_feature_sets; gc.collect()\n",
    "            continue\n",
    "        \n",
    "        # Check if data preparation was successful before proceeding to scenarios\n",
    "        if X_train is None or X_train_scaled is None:\n",
    "            print(f\"  ERROR: X data splits not available after preparation for window {window_minutes} min. Skipping model training for this window.\")\n",
    "            # Error already logged if exception occurred, this is a fallback\n",
    "            error_msg = 'X_data_missing_post_successful_prep_block'\n",
    "            all_results_list.append({'model_type': 'RandomForestClassifier', 'time_window': window_minutes, 'ablation_scenario': 'baseline', 'error': error_msg})\n",
    "            all_results_list.append({'model_type': 'LogisticRegression', 'time_window': window_minutes, 'ablation_scenario': 'baseline', 'error': error_msg})\n",
    "            if window_minutes == 60:\n",
    "                for modality_name in MODALITY_PREFIX_GROUPS.keys():\n",
    "                    all_results_list.append({'model_type': 'RandomForestClassifier', 'time_window': window_minutes, 'ablation_scenario': f'exclude_{modality_name}', 'error': error_msg})\n",
    "            del X_train, y_train, X_valid, y_valid, X_test, y_test, X_train_scaled, X_valid_scaled, X_test_scaled, scaler, all_feature_cols, modality_feature_sets; gc.collect()\n",
    "            continue\n",
    "\n",
    "\n",
    "        scenarios_to_run = [\"baseline\"]\n",
    "        rf_ablation_active = False\n",
    "        if window_minutes == 60:\n",
    "            scenarios_to_run.extend([f\"exclude_{name}\" for name in MODALITY_PREFIX_GROUPS.keys()])\n",
    "            rf_ablation_active = True\n",
    "            print(f\"\\n  >>> RF Ablation Study ACTIVE for {window_minutes} min window <<<\")\n",
    "        else:\n",
    "            print(f\"\\n  >>> Baseline Only for {window_minutes} min window <<<\")\n",
    "\n",
    "        for scenario in scenarios_to_run:\n",
    "            scenario_start_time = time.time()\n",
    "            print(f\"\\n  --- Running Scenario: {scenario} (Window {window_minutes} min) ---\")\n",
    "\n",
    "            current_feature_cols_scenario = list(all_feature_cols)\n",
    "            if scenario != \"baseline\":\n",
    "                modality_to_exclude = scenario.split(\"exclude_\")[-1]\n",
    "                features_to_exclude = modality_feature_sets.get(modality_to_exclude, [])\n",
    "                if features_to_exclude:\n",
    "                    current_feature_cols_scenario = [col for col in all_feature_cols if col not in features_to_exclude]\n",
    "                    print(f\"    Excluding {len(features_to_exclude)} '{modality_to_exclude}' features. Using {len(current_feature_cols_scenario)} features.\")\n",
    "                else:\n",
    "                    print(f\"    WARNING: No features for modality '{modality_to_exclude}' to exclude. Using all features.\")\n",
    "            else:\n",
    "                 print(f\"    Using all {len(current_feature_cols_scenario)} features (baseline).\")\n",
    "\n",
    "            if not current_feature_cols_scenario:\n",
    "                print(f\"    ERROR: Feature list for scenario '{scenario}' is empty. Skipping training.\")\n",
    "                if scenario == \"baseline\" or (rf_ablation_active and scenario.startswith(\"exclude_\")):\n",
    "                    all_results_list.append({'model_type': 'RandomForestClassifier', 'time_window': window_minutes, 'ablation_scenario': scenario, 'error': 'EmptyFeatureList'})\n",
    "                if scenario == \"baseline\":\n",
    "                    all_results_list.append({'model_type': 'LogisticRegression', 'time_window': window_minutes, 'ablation_scenario': scenario, 'error': 'EmptyFeatureList'})\n",
    "                continue\n",
    "\n",
    "            X_train_scenario = X_train[current_feature_cols_scenario]\n",
    "            X_valid_scenario = X_valid[current_feature_cols_scenario]\n",
    "            X_test_scenario = X_test[current_feature_cols_scenario]\n",
    "            \n",
    "            feature_indices = [all_feature_cols.index(col) for col in current_feature_cols_scenario]\n",
    "            X_train_scaled_scenario = X_train_scaled[:, feature_indices]\n",
    "            X_valid_scaled_scenario = X_valid_scaled[:, feature_indices]\n",
    "            X_test_scaled_scenario = X_test_scaled[:, feature_indices]\n",
    "\n",
    "            # === Random Forest ===\n",
    "            if scenario == \"baseline\" or (rf_ablation_active and scenario.startswith(\"exclude_\")):\n",
    "                print(\"\\n    --- Random Forest ---\")\n",
    "                rf_model = RandomForestClassifier(n_estimators=250, class_weight='balanced', random_state=RANDOM_STATE, n_jobs=-1, max_depth=20, min_samples_leaf=10, oob_score=False, verbose=0)\n",
    "                rf_results = {'model_type': 'RandomForestClassifier', 'time_window': window_minutes, 'ablation_scenario': scenario}\n",
    "                try:\n",
    "                    print(f\"      Training RF model...\")\n",
    "                    rf_model.fit(X_train_scenario, y_train)\n",
    "                    \n",
    "                    valid_metrics_rf = evaluate_model(rf_model, X_valid_scenario, y_valid, 'RF', window_minutes, scenario, 'valid')\n",
    "                    rf_results.update(valid_metrics_rf)\n",
    "                    test_metrics_rf = evaluate_model(rf_model, X_test_scenario, y_test, 'RF', window_minutes, scenario, 'test')\n",
    "                    rf_results.update(test_metrics_rf)\n",
    "                    rf_results['error'] = None\n",
    "                    print(f\"      RF ({scenario}): Test PR AUC: {rf_results.get('pr_auc_test', np.nan):.4f}, ROC AUC: {rf_results.get('roc_auc_test', np.nan):.4f}\")\n",
    "\n",
    "                    save_feature_importance(rf_model, current_feature_cols_scenario, 'RF', window_minutes, FEATURE_IMPORTANCE_DIR, scenario)\n",
    "\n",
    "                    if scenario == \"baseline\": # SHAP only for baseline RF\n",
    "                        print(f\"      Calculating SHAP values for baseline RF model...\")\n",
    "                        explainer = shap.TreeExplainer(rf_model)\n",
    "                        X_valid_shap_df = pd.DataFrame(X_valid_scenario, columns=current_feature_cols_scenario) # SHAP prefers DataFrame\n",
    "                        shap_values = explainer.shap_values(X_valid_shap_df)\n",
    "                        positive_class_index = np.where(rf_model.classes_ == 1)[0][0]\n",
    "                        shap_values_positive = shap_values[positive_class_index]\n",
    "                        \n",
    "                        save_shap_summary(shap_values_positive, current_feature_cols_scenario, 'RF', window_minutes, SHAP_RESULTS_DIR, scenario)\n",
    "                        \n",
    "                        shap.summary_plot(shap_values_positive, X_valid_shap_df, plot_type=\"bar\", show=False)\n",
    "                        plt.title(f\"SHAP Importance (Mean Abs) - RF Baseline {window_minutes}min\")\n",
    "                        plt.tight_layout(); plt.savefig(SHAP_RESULTS_DIR / f\"RF_shap_summary_bar_{window_minutes}min_baseline.png\", bbox_inches='tight'); plt.close()\n",
    "                        \n",
    "                        shap.summary_plot(shap_values_positive, X_valid_shap_df, show=False)\n",
    "                        plt.title(f\"SHAP Detailed Summary - RF Baseline {window_minutes}min\")\n",
    "                        plt.tight_layout(); plt.savefig(SHAP_RESULTS_DIR / f\"RF_shap_summary_beeswarm_{window_minutes}min_baseline.png\", bbox_inches='tight'); plt.close()\n",
    "                        del explainer, shap_values, shap_values_positive, X_valid_shap_df; gc.collect()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"      Error during RF for scenario '{scenario}', window {window_minutes} min: {e}\")\n",
    "                    traceback.print_exc()\n",
    "                    rf_results['error'] = f'TrainEvalError: {e}'\n",
    "                finally:\n",
    "                    all_results_list.append(rf_results)\n",
    "                    if 'rf_model' in locals(): del rf_model; gc.collect()\n",
    "            \n",
    "            # === Logistic Regression ===\n",
    "            if scenario == \"baseline\":\n",
    "                print(\"\\n    --- Logistic Regression ---\")\n",
    "                lr_model = LogisticRegression(class_weight='balanced', random_state=RANDOM_STATE, solver='liblinear', max_iter=1000)\n",
    "                lr_results = {'model_type': 'LogisticRegression', 'time_window': window_minutes, 'ablation_scenario': scenario}\n",
    "                try:\n",
    "                    print(f\"      Training LR model...\")\n",
    "                    lr_model.fit(X_train_scaled_scenario, y_train)\n",
    "\n",
    "                    valid_metrics_lr = evaluate_model(lr_model, X_valid_scaled_scenario, y_valid, 'LR', window_minutes, scenario, 'valid')\n",
    "                    lr_results.update(valid_metrics_lr)\n",
    "                    test_metrics_lr = evaluate_model(lr_model, X_test_scaled_scenario, y_test, 'LR', window_minutes, scenario, 'test')\n",
    "                    lr_results.update(test_metrics_lr)\n",
    "                    lr_results['error'] = None\n",
    "                    print(f\"      LR ({scenario}): Test PR AUC: {lr_results.get('pr_auc_test', np.nan):.4f}, ROC AUC: {lr_results.get('roc_auc_test', np.nan):.4f}\")\n",
    "                    \n",
    "                    save_feature_importance(lr_model, current_feature_cols_scenario, 'LR', window_minutes, FEATURE_IMPORTANCE_DIR, scenario)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"      Error during LR for scenario '{scenario}', window {window_minutes} min: {e}\")\n",
    "                    traceback.print_exc()\n",
    "                    lr_results['error'] = f'TrainEvalError: {e}'\n",
    "                finally:\n",
    "                    all_results_list.append(lr_results)\n",
    "                    if 'lr_model' in locals(): del lr_model; gc.collect()\n",
    "\n",
    "            del X_train_scenario, X_valid_scenario, X_test_scenario\n",
    "            del X_train_scaled_scenario, X_valid_scaled_scenario, X_test_scaled_scenario\n",
    "            gc.collect()\n",
    "            print(f\"  --- Finished scenario '{scenario}'. Took {time.time() - scenario_start_time:.2f} seconds ---\")\n",
    "            # --- End of scenario loop ---\n",
    "\n",
    "        if 'X_train' in locals(): del X_train, y_train, X_valid, y_valid, X_test, y_test; gc.collect()\n",
    "        if 'X_train_scaled' in locals(): del X_train_scaled, X_valid_scaled, X_test_scaled; gc.collect()\n",
    "        if 'scaler' in locals(): del scaler; gc.collect()\n",
    "        if 'all_feature_cols' in locals(): del all_feature_cols; gc.collect()\n",
    "        if 'modality_feature_sets' in locals(): del modality_feature_sets; gc.collect()\n",
    "        \n",
    "        print(f\"----- Finished window {window_minutes} min | Total Time: {time.time() - window_start_time:.2f} seconds -----\")\n",
    "        # --- End of window loop ---\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"===== Processing Final RF & LR (Baseline All Windows), RF (Ablation 60min) Results =====\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    if all_results_list:\n",
    "        df_final_results = pd.DataFrame(all_results_list)\n",
    "        column_order = [\n",
    "            'time_window', 'ablation_scenario', 'model_type',\n",
    "            'pr_auc_valid', 'roc_auc_valid', 'f1_score_valid', 'precision_valid', 'recall_valid', 'accuracy_valid',\n",
    "            'pr_auc_test', 'roc_auc_test', 'f1_score_test', 'precision_test', 'recall_test', 'accuracy_test',\n",
    "            'error'\n",
    "        ]\n",
    "        available_cols = [c for c in column_order if c in df_final_results.columns]\n",
    "        error_cols = [c for c in df_final_results.columns if c.startswith('error_') and c != 'error']\n",
    "        final_cols = available_cols + error_cols\n",
    "        df_final_results = df_final_results[final_cols]\n",
    "\n",
    "        scenario_order_list = ['baseline'] + [f\"exclude_{name}\" for name in MODALITY_PREFIX_GROUPS.keys()] + ['error_scenario']\n",
    "        df_final_results['ablation_scenario'] = df_final_results['ablation_scenario'].fillna('error_scenario')\n",
    "        df_final_results['ablation_scenario_cat'] = pd.Categorical(\n",
    "            df_final_results['ablation_scenario'], categories=scenario_order_list, ordered=True\n",
    "        )\n",
    "        \n",
    "        df_final_results_sorted = df_final_results.sort_values(\n",
    "            by=['time_window', 'ablation_scenario_cat', 'model_type', 'pr_auc_test'],\n",
    "            ascending=[True, True, True, False],\n",
    "            na_position='last'\n",
    "        ).drop(columns=['ablation_scenario_cat'])\n",
    "\n",
    "        print(\"\\n--- Aggregated Results Table ---\")\n",
    "        print(df_final_results_sorted.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "        final_results_save_path = EVALUATION_RESULTS_DIR / FINAL_RESULTS_FILENAME\n",
    "        try:\n",
    "            df_final_results_sorted.to_csv(final_results_save_path, index=False, float_format='%.5f')\n",
    "            print(f\"\\nSaved aggregated results table to: {final_results_save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving aggregated results table: {e}\")\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(\"\\nNo results were generated.\")\n",
    "\n",
    "    script_duration = time.time() - script_start_time\n",
    "    print(f\"\\n--- Full Script Finished | Total Time: {script_duration:.2f} seconds ({script_duration/60:.2f} minutes) ---\")\n",
    "    print(f\"--- Baseline feature importance files saved in: {FEATURE_IMPORTANCE_DIR} ---\")\n",
    "    print(f\"--- Baseline RF SHAP summary CSVs and plots saved in: {SHAP_RESULTS_DIR} ---\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
